{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Eksperimen Machine Learning - Transactions Dataset\n## David Dewanto\n\nNotebook ini berisi eksperimen lengkap untuk:\n1. Data Loading\n2. Exploratory Data Analysis (EDA)\n3. Data Preprocessing\n\nDataset: Transactions (Fraud Detection)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load raw dataset\ndf_raw = pd.read_csv('../transactions.csv')\n\nprint(\"Dataset loaded successfully!\")\nprint(f\"Shape: {df_raw.shape}\")\nprint(f\"\\nFirst 5 rows:\")\ndf_raw.head()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*50)\n",
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Check Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_values = df_raw.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\"*50)\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates\n",
    "duplicates = df_raw.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    print(df_raw[df_raw.duplicated()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.3 Target Distribution"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Target distribution\nprint(\"Fraud Distribution:\")\nprint(\"=\"*50)\nprint(df_raw['is_fraud'].value_counts())\nprint(f\"\\nPercentage:\")\nprint(df_raw['is_fraud'].value_counts(normalize=True) * 100)\n\n# Visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Count plot\ndf_raw['is_fraud'].value_counts().plot(kind='bar', ax=ax1, color='skyblue')\nax1.set_title('Fraud Distribution (Count)', fontsize=14, fontweight='bold')\nax1.set_xlabel('is_fraud')\nax1.set_ylabel('Count')\nax1.tick_params(axis='x', rotation=45)\n\n# Pie chart\ndf_raw['is_fraud'].value_counts().plot(kind='pie', ax=ax2, autopct='%1.1f%%')\nax2.set_title('Fraud Distribution (Percentage)', fontsize=14, fontweight='bold')\nax2.set_ylabel('')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Feature Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Distribution of numerical features\nnumerical_features = ['account_age_days', 'total_transactions_user', 'avg_amount_user', 'amount', 'shipping_distance_km']\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.ravel()\n\nfor idx, col in enumerate(numerical_features):\n    axes[idx].hist(df_raw[col], bins=20, color='steelblue', edgecolor='black')\n    axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n    axes[idx].set_xlabel(col)\n    axes[idx].set_ylabel('Frequency')\n\n# Hide the extra subplot\naxes[5].set_visible(False)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Box Plots for Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Box plots for outlier detection\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\naxes = axes.ravel()\n\nfor idx, col in enumerate(numerical_features):\n    sns.boxplot(data=df_raw, y=col, ax=axes[idx], color='lightblue')\n    axes[idx].set_title(f'Box Plot: {col}', fontsize=12, fontweight='bold')\n\n# Hide the extra subplot\naxes[5].set_visible(False)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = df_raw[numerical_features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.7 Pair Plot by Fraud Status"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Pair plot (using a sample for performance with large dataset)\nsample_df = df_raw.sample(n=min(1000, len(df_raw)), random_state=42)\nsns.pairplot(sample_df[numerical_features + ['is_fraud']], hue='is_fraud', height=2.5, diag_kind='kde')\nplt.suptitle('Pair Plot of Transaction Features by Fraud Status (Sample)', y=1.02, fontsize=16, fontweight='bold')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 3.8 Feature Statistics by Fraud Status"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Group statistics by fraud status\nprint(\"Feature Statistics by Fraud Status:\")\nprint(\"=\"*80)\nprint(df_raw.groupby('is_fraud')[numerical_features].mean())\n\nprint(\"\\nStandard Deviation by Fraud Status:\")\nprint(\"=\"*80)\nprint(df_raw.groupby('is_fraud')[numerical_features].std())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create a Copy for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df = df_raw.copy()\n",
    "print(f\"Working with a copy of the dataset. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Handle Missing Values (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and handle missing values\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"Handling missing values...\")\n",
    "    # For numerical features, fill with median\n",
    "    for col in numerical_features:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "    print(\"Missing values handled.\")\n",
    "else:\n",
    "    print(\"No missing values found. Proceeding...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Remove Duplicates (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "before_duplicates = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "after_duplicates = df.shape[0]\n",
    "\n",
    "print(f\"Removed {before_duplicates - after_duplicates} duplicate rows.\")\n",
    "print(f\"Dataset shape after removing duplicates: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create new features\ndf['amount_transactions_product'] = df['amount'] * df['total_transactions_user']\ndf['amount_avg_product'] = df['amount'] * df['avg_amount_user']\ndf['amount_avg_ratio'] = df['amount'] / df['avg_amount_user']\ndf['shipping_age_ratio'] = df['shipping_distance_km'] / (df['account_age_days'] + 1)\n\nprint(\"New features created:\")\nprint(\"- amount_transactions_product\")\nprint(\"- amount_avg_product\")\nprint(\"- amount_avg_ratio\")\nprint(\"- shipping_age_ratio\")\nprint(f\"\\nNew dataset shape: {df.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Label Encoding for Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Label encoding for is_fraud\nlabel_encoder = LabelEncoder()\ndf['target_encoded'] = label_encoder.fit_transform(df['is_fraud'])\n\nprint(\"Label Encoding Mapping:\")\nfor idx, label in enumerate(label_encoder.classes_):\n    print(f\"{label}: {idx}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare features for scaling\nfeature_columns = ['account_age_days', 'total_transactions_user', 'avg_amount_user', \n                   'amount', 'shipping_distance_km',\n                   'amount_transactions_product', 'amount_avg_product', \n                   'amount_avg_ratio', 'shipping_age_ratio']\n\n# Create a scaler\nscaler = StandardScaler()\n\n# Fit and transform the features\ndf_scaled = df.copy()\ndf_scaled[feature_columns] = scaler.fit_transform(df[feature_columns])\n\nprint(\"Features scaled using StandardScaler\")\nprint(\"\\nScaled features (first 5 rows):\")\nprint(df_scaled[feature_columns].head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = df_scaled[feature_columns]\n",
    "y = df_scaled['target_encoded']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train-Test Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nClass distribution in testing set:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save the preprocessed dataset\ndf_preprocessed = df_scaled.copy()\ndf_preprocessed.to_csv('transactions_preprocessing.csv', index=False)\n\nprint(\"Preprocessed dataset saved to: transactions_preprocessing.csv\")\nprint(f\"Shape: {df_preprocessed.shape}\")\nprint(\"\\nFirst few rows of preprocessed data:\")\nprint(df_preprocessed.head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Original dataset shape: {df_raw.shape}\")\n",
    "print(f\"Preprocessed dataset shape: {df_preprocessed.shape}\")\n",
    "print(f\"\\nOriginal features: {len(numerical_features)}\")\n",
    "print(f\"Total features after engineering: {len(feature_columns)}\")\n",
    "print(f\"New features created: {len(feature_columns) - len(numerical_features)}\")\n",
    "print(f\"\\nMissing values: {df_preprocessed.isnull().sum().sum()}\")\n",
    "print(f\"Duplicates: {df_preprocessed.duplicated().sum()}\")\n",
    "print(f\"\\nTraining samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "print(f\"\\nTarget classes: {label_encoder.classes_.tolist()}\")\n",
    "print(\"\\nPreprocessing steps completed:\")\n",
    "print(\"✓ Data loading\")\n",
    "print(\"✓ Exploratory Data Analysis\")\n",
    "print(\"✓ Missing value handling\")\n",
    "print(\"✓ Duplicate removal\")\n",
    "print(\"✓ Feature engineering\")\n",
    "print(\"✓ Label encoding\")\n",
    "print(\"✓ Feature scaling\")\n",
    "print(\"✓ Train-test split\")\n",
    "print(\"✓ Data saved\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}